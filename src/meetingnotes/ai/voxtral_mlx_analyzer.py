"""
Module d'analyse audio avec les mod√®les Voxtral via MLX (optimis√© Apple Silicon).

Ce module utilise MLX pour :
- L'analyse audio multilingue directe optimis√©e pour Apple Silicon
- La g√©n√©ration de r√©sum√©s structur√©s depuis l'audio
- Le traitement des fichiers longs avec d√©coupage automatique
- Performance acc√©l√©r√©e sur Mac M1/M2/M3

D√©pendances:
    - mlx: Framework ML optimis√© Apple Silicon
    - mlx-voxtral: Impl√©mentation MLX des mod√®les Voxtral
    - pydub: Traitement audio pour le d√©coupage
"""

import os
import tempfile
import time
from pydub import AudioSegment
from typing import List, Dict, Tuple, Optional
import math

from .prompts_config import VoxtralPrompts
from ..utils import format_duration, token_tracker


class VoxtralMLXAnalyzer:
    """
    Analyseur audio intelligent utilisant les mod√®les Voxtral via MLX.
    
    Cette classe g√®re l'analyse et compr√©hension audio directe via MLX
    avec support du d√©coupage intelligent pour les fichiers longs.
    Optimis√© pour les processeurs Apple Silicon.
    """
    
    def __init__(self, model_name: str = "mistralai/Voxtral-Mini-3B-2507"):
        """
        Initialise l'analyseur MLX Voxtral.
        
        Args:
            model_name (str): Nom du mod√®le ("mistralai/Voxtral-Mini-3B-2507" ou "mistralai/Voxtral-Small-24B-2507")
        """
        self.model_name = model_name
        self.max_duration_minutes = 15  # Limite raisonnable pour √©viter les timeouts
        
        # Mapping des noms de mod√®les
        model_mapping = {
            "Voxtral-Mini-3B-2507": "mistralai/Voxtral-Mini-3B-2507",
            "Voxtral-Small-24B-2507": "mistralai/Voxtral-Small-24B-2507"
        }
        
        self.model_name = model_mapping.get(model_name, model_name)
        
        self.model = None
        self.processor = None
        
        print(f"üîÑ Initialisation de l'analyseur MLX Voxtral...")
        print(f"ü§ñ Mod√®le: {self.model_name}")
        print(f"üöÄ Backend: MLX (Apple Silicon optimis√©)")
    
    def _load_model(self):
        """
        Charge le mod√®le Voxtral avec MLX.
        """
        if self.model is None or self.processor is None:
            try:
                from mlx_voxtral import load_voxtral_model, VoxtralProcessor
                import mlx.core as mx
                
                print(f"üîÑ Chargement du mod√®le MLX...")
                
                # Utiliser directement le nom du mod√®le (peut √™tre quantifi√©)
                mlx_model_id = self.model_name
                print(f"üîÑ Utilisation du mod√®le MLX: {mlx_model_id}")
                
                self.model, self.config = load_voxtral_model(mlx_model_id, dtype=mx.bfloat16)
                self.processor = VoxtralProcessor.from_pretrained(mlx_model_id)
                
                print(f"‚úÖ Mod√®le MLX charg√© avec succ√®s")
                
            except ImportError as e:
                raise ImportError(f"Erreur d'import mlx-voxtral. V√©rifiez l'installation:\npip install mlx mlx-voxtral\nErreur: {e}")
            except Exception as e:
                raise RuntimeError(f"Erreur lors du chargement du mod√®le MLX: {e}")
    
    def _get_audio_duration(self, wav_path: str) -> float:
        """
        Obtient la dur√©e d'un fichier audio en minutes.
        
        Args:
            wav_path (str): Chemin vers le fichier audio
            
        Returns:
            float: Dur√©e en minutes
        """
        audio = AudioSegment.from_file(wav_path)
        return len(audio) / (1000 * 60)  # Conversion ms -> minutes
    
    def _extract_audio_chunk(self, wav_path: str, start_time: float, end_time: float) -> str:
        """
        Extrait un chunk audio entre deux timestamps.
        
        Args:
            wav_path (str): Chemin vers le fichier audio source
            start_time (float): D√©but en secondes
            end_time (float): Fin en secondes
            
        Returns:
            str: Chemin vers le fichier chunk temporaire
        """
        audio = AudioSegment.from_file(wav_path)
        
        start_ms = int(start_time * 1000)
        end_ms = int(end_time * 1000)
        
        chunk = audio[start_ms:end_ms]
        
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_chunk:
            chunk_path = tmp_chunk.name
        
        chunk.export(chunk_path, format="wav")
        return chunk_path
    
    def _adjust_diarization_timestamps(self, reference_speakers_data: str, start_offset_seconds: float, chunk_duration_seconds: float) -> str:
        """
        Ajuste les timestamps de diarisation selon l'offset du segment audio.
        Ne garde que les segments qui sont r√©ellement dans ce chunk.
        
        Args:
            reference_speakers_data (str): Donn√©es de diarisation avec balises
            start_offset_seconds (float): D√©calage en secondes du d√©but du segment
            chunk_duration_seconds (float): Dur√©e du chunk en secondes
            
        Returns:
            str: Diarisation ajust√©e avec les nouveaux timestamps
        """
        if not reference_speakers_data or not reference_speakers_data.strip():
            return reference_speakers_data
        
        adjusted_lines = []
        
        for line in reference_speakers_data.split('\n'):
            if '<d√©but>' in line and '<fin>' in line:
                # Extraire les timestamps actuels
                try:
                    start_tag_start = line.find('<d√©but>') + len('<d√©but>')
                    start_tag_end = line.find('</d√©but>')
                    end_tag_start = line.find('<fin>') + len('<fin>')
                    end_tag_end = line.find('</fin>')
                    
                    original_start = float(line[start_tag_start:start_tag_end])
                    original_end = float(line[end_tag_start:end_tag_end])
                    
                    # V√©rifier si le segment a une intersection avec ce chunk
                    chunk_start = start_offset_seconds
                    chunk_end = start_offset_seconds + chunk_duration_seconds
                    
                    # Segment compl√®tement avant ou apr√®s ce chunk ? Ignorer
                    if original_end <= chunk_start or original_start >= chunk_end:
                        continue
                    
                    # Calculer l'intersection avec le chunk
                    intersect_start = max(original_start, chunk_start)
                    intersect_end = min(original_end, chunk_end)
                    
                    # Ajuster les timestamps par rapport au d√©but du chunk
                    adjusted_start = intersect_start - start_offset_seconds
                    adjusted_end = intersect_end - start_offset_seconds
                    
                    # S'assurer que les timestamps sont positifs et dans les limites du chunk
                    adjusted_start = max(0, adjusted_start)
                    adjusted_end = min(chunk_duration_seconds, adjusted_end)
                    
                    # Seulement inclure si on a encore une dur√©e significative (>0.1s)
                    if adjusted_end - adjusted_start > 0.1:
                        # Reconstituer la ligne avec les nouveaux timestamps
                        adjusted_line = line[:start_tag_start] + f"{adjusted_start:.3f}" + line[start_tag_end:end_tag_start] + f"{adjusted_end:.3f}" + line[end_tag_end:]
                        adjusted_lines.append(adjusted_line)
                        
                except (ValueError, IndexError) as e:
                    # En cas d'erreur de parsing, garder la ligne originale
                    print(f"‚ö†Ô∏è Erreur ajustement timestamp: {e}")
                    adjusted_lines.append(line)
            else:
                # Ligne sans timestamp, garder telle quelle
                adjusted_lines.append(line)
        
        return '\n'.join(adjusted_lines)
    
    def _analyze_audio_chunk_mlx(self, audio_path: str, prompt: str) -> str:
        """
        Analyse directe d'un chunk audio via MLX avec inf√©rence chat multimodale.
        
        Utilise l'approche directe audio-chat comme dans l'exemple voxtral_chat.py
        pour √©viter la transcription interm√©diaire.
        
        Args:
            audio_path (str): Chemin vers le fichier audio
            prompt (str): Prompt d'analyse
            
        Returns:
            str: R√©sultat de l'analyse
        """
        try:
            # S'assurer que le mod√®le est charg√©
            self._load_model()
            
            if not os.path.exists(audio_path):
                return f"‚ùå Fichier audio non trouv√©: {audio_path}"
            
            # Construire la conversation avec contenu multimodal (texte + audio)
            conversation = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "audio", "url": audio_path}
                    ]
                }
            ]
            
            # Appliquer le template de chat pour formater les inputs
            inputs = self.processor.apply_chat_template(conversation, return_tensors="mlx")
            
            # G√©n√©rer la r√©ponse avec le mod√®le MLX
            outputs = self.model.generate(
                inputs["input_ids"],
                input_features=inputs.get("input_features"),  # Features audio si disponibles
                max_new_tokens=4000,
                do_sample=False,  # D√©terministe pour plus de coh√©rence
                temperature=0.1
            )
            # D√©coder la r√©ponse en excluant les tokens d'entr√©e
            # Convertir de MLX vers numpy/list avant d√©codage
            import mlx.core as mx
            input_tokens_count = inputs["input_ids"].shape[1]
            output_tokens = outputs[0][input_tokens_count:]
            output_tokens_count = len(output_tokens)
            output_tokens_numpy = mx.array(output_tokens).tolist()
            
            response = self.processor.tokenizer.decode(
                output_tokens_numpy,
                skip_special_tokens=True
            ).strip()
            
            token_tracker.add_chunk_tokens(input_tokens_count, output_tokens_count)
            
            if not response or len(response.strip()) < 10:
                return "‚ùå R√©ponse vide ou trop courte du mod√®le MLX"
            
            return response
            
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            print(f"‚ùå Erreur d√©taill√©e MLX:")
            print(error_details)
            return f"‚ùå Erreur analyse MLX: {str(e)}"
    
    def analyze_audio_chunks_mlx(
        self, 
        wav_path: str, 
        language: str = "french", 
        selected_sections: list = None,
        chunk_duration_minutes: int = 15,
        reference_speakers_data=None
    ) -> Dict[str, str]:
        """
        Analyse directe de l'audio par chunks via les mod√®les MLX.
        
        Args:
            wav_path (str): Chemin vers le fichier audio
            language (str): Langue attendue
            selected_sections (list): Sections du r√©sum√© √† inclure
            chunk_duration_minutes (int): Dur√©e des chunks en minutes
            reference_speakers_data: Donn√©es des locuteurs de r√©f√©rence identifi√©s
            
        Returns:
            Dict[str, str]: R√©sultats avec 'transcription' (analyse concat√©n√©e)
        """
        # Mesurer le temps total de traitement
        total_start_time = time.time()
        
        # Initialize token tracker for MLX mode
        token_tracker.reset()
        token_tracker.set_mode("MLX")
        
        # Obtenir la dur√©e audio
        duration = self._get_audio_duration(wav_path)
        print(f"üéµ Dur√©e audio: {duration:.1f} minutes")
        
        # Si le fichier est plus court que la limite, traiter en un seul chunk
        if duration <= chunk_duration_minutes:
            print(f"üìÑ Fichier court, traitement en un seul chunk")
            
            # Ajuster les timestamps de diarisation pour le fichier complet
            adjusted_speaker_context = ""
            if reference_speakers_data:
                adjusted_speaker_context = self._adjust_diarization_timestamps(
                    reference_speakers_data, 0, duration * 60
                )
            else:
                adjusted_speaker_context = ""
            
            # Utiliser audio instruct mode pour analyse directe depuis la config centralis√©e
            sections_list = selected_sections if selected_sections else ["resume_executif"]
            
            # Pas de chunk_info pour un fichier complet
            prompt_text = VoxtralPrompts.get_meeting_summary_prompt(sections_list, adjusted_speaker_context, None, None)
            
            print(f"üîÑ D√©but analyse MLX du fichier {wav_path}")
            chunk_start_time = time.time()
            chunk_summary = self._analyze_audio_chunk_mlx(wav_path, prompt_text)
            chunk_duration = time.time() - chunk_start_time
            
            print(f"‚úÖ Analyse termin√©e en {format_duration(chunk_duration)}")
            
            # Calculer et afficher le temps total
            total_duration = time.time() - total_start_time
            print(f"‚è±Ô∏è Analyse directe MLX totale en {format_duration(total_duration)} pour {duration:.1f}min d'audio")
            
            return {"transcription": chunk_summary}
        
        # Cr√©er des chunks de temps fixes avec overlap de 10 secondes
        chunk_duration = chunk_duration_minutes * 60  # Convertir en secondes
        overlap_duration = 10  # 10 secondes d'overlap
        chunks = []
        current_time = 0
        
        while current_time < duration * 60:
            end_time = min(current_time + chunk_duration, duration * 60)
            chunks.append((current_time, end_time))
            
            # Pour le prochain chunk, commencer 10 secondes avant la fin du chunk actuel
            # sauf si c'est le dernier chunk
            if end_time < duration * 60:
                current_time = max(0, end_time - overlap_duration)
            else:
                break
        
        print(f"üì¶ Division en {len(chunks)} chunks de {chunk_duration_minutes} minutes")
        
        # Liste pour stocker les r√©sum√©s de chaque chunk
        chunk_summaries = []
        
        for i, (start_time, end_time) in enumerate(chunks):
            print(f"üéØ Traitement du chunk {i+1}/{len(chunks)} ({start_time/60:.1f}-{end_time/60:.1f}min)")
            
            # Mesurer le temps de traitement du chunk
            chunk_start_time = time.time()
            
            # Extraire le chunk audio
            chunk_path = self._extract_audio_chunk(wav_path, start_time, end_time)
            
            try:
                # Ajuster la diarisation selon l'offset temporel du chunk
                adjusted_speaker_context = ""
                if reference_speakers_data:
                    chunk_duration_seconds = end_time - start_time
                    adjusted_speaker_context = self._adjust_diarization_timestamps(
                        reference_speakers_data, start_time, chunk_duration_seconds
                    )
                
                # Utiliser audio instruct mode pour analyse directe depuis la config centralis√©e
                sections_list = selected_sections if selected_sections else ["resume_executif"]
                
                # Indiquer que c'est un segment d'un audio plus long seulement s'il y a plusieurs chunks
                chunk_info = f"SEGMENT {i+1}/{len(chunks)} ({start_time/60:.1f}-{end_time/60:.1f}min)" if len(chunks) > 1 else None
                prompt_text = VoxtralPrompts.get_meeting_summary_prompt(sections_list, adjusted_speaker_context, chunk_info, None)
                
                
                chunk_summary = self._analyze_audio_chunk_mlx(chunk_path, prompt_text)
                
                chunk_summaries.append(f"## Segment {i+1} ({start_time/60:.1f}-{end_time/60:.1f}min)\n\n{chunk_summary}")
                
                # Calculer et afficher le temps de traitement
                chunk_duration = time.time() - chunk_start_time
                print(f"‚úÖ Chunk {i+1} analys√© en {format_duration(chunk_duration)}")
                
            except Exception as e:
                print(f"‚ùå Erreur chunk {i+1}: {e}")
                chunk_summaries.append(f"**Segment {i+1}:** Erreur de traitement")
            finally:
                # Nettoyer le fichier temporaire du chunk
                import os
                if os.path.exists(chunk_path):
                    os.remove(chunk_path)
        
        # Traitement final selon le nombre de chunks
        if chunk_summaries:
            if len(chunk_summaries) == 1:
                # Un seul chunk, pas besoin de synth√®se
                final_analysis = chunk_summaries[0]
                print(f"‚úÖ Analyse directe MLX termin√©e")
            else:
                # Plusieurs chunks : synth√®se finale en mode texte
                print(f"üîÑ Synth√®se finale MLX en mode texte des {len(chunk_summaries)} segments...")
                combined_content = "\n\n".join(chunk_summaries)
                final_analysis = self._synthesize_chunks_final_mlx(combined_content, selected_sections)
                print(f"‚úÖ Analyse MLX avec synth√®se finale termin√©e avec {len(chunk_summaries)} segments")
        else:
            final_analysis = "Aucune analyse disponible."
        
        # Calculer et afficher le temps total
        total_duration = time.time() - total_start_time
        print(f"‚è±Ô∏è Analyse directe MLX totale en {format_duration(total_duration)} pour {duration:.1f}min d'audio")
        
        # Print token usage summary
        token_tracker.print_summary()
        
        return {"transcription": final_analysis}
    
    def _synthesize_chunks_final_mlx(self, combined_content: str, selected_sections: list) -> str:
        """
        Fait une synth√®se finale en mode texte via MLX de tous les segments analys√©s.
        
        Args:
            combined_content (str): Contenu combin√© de tous les segments
            selected_sections (list): Sections s√©lectionn√©es pour le r√©sum√©
            
        Returns:
            str: Synth√®se finale structur√©e
        """
        try:
            # S'assurer que le mod√®le est charg√©
            self._load_model()
            
            # Cr√©er le prompt pour la synth√®se finale
            sections_text = ""
            if selected_sections:
                from .prompts_config import VoxtralPrompts
                for section_key in selected_sections:
                    if section_key in VoxtralPrompts.AVAILABLE_SECTIONS:
                        section = VoxtralPrompts.AVAILABLE_SECTIONS[section_key]
                        sections_text += f"\n{section['title']}\n{section['description']}\n"
            
            synthesis_prompt = f"""Voici les analyses d√©taill√©es de plusieurs segments d'une r√©union :

{combined_content}

INSTRUCTION CRITIQUE - LANGUE DE R√âPONSE :
- D√âTECTE la langue utilis√©e dans les segments ci-dessus
- R√âPONDS OBLIGATOIREMENT dans cette m√™me langue d√©tect√©e
- Si les segments sont en fran√ßais ‚Üí r√©ponds en fran√ßais
- Si les segments sont en anglais ‚Üí r√©ponds en anglais

Synth√©tise maintenant ces analyses en un r√©sum√© global coh√©rent et structur√© selon les sections demand√©es :{sections_text}

Fournis une synth√®se unifi√©e qui combine et r√©sume les informations de tous les segments de mani√®re coh√©rente."""

            # Construire la conversation pour la synth√®se textuelle 
            conversation = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": synthesis_prompt}
                    ]
                }
            ]
            
            # Appliquer le template de chat
            inputs = self.processor.apply_chat_template(conversation, return_tensors="mlx")
            
            # G√©n√©rer la synth√®se avec MLX
            outputs = self.model.generate(
                inputs["input_ids"],
                max_new_tokens=4000,
                do_sample=False,
                temperature=0.1
            )
            
            # D√©coder la r√©ponse
            # Convertir de MLX vers numpy/list avant d√©codage
            import mlx.core as mx
            input_tokens_count = inputs["input_ids"].shape[1]
            synthesis_tokens = outputs[0][input_tokens_count:]
            synthesis_tokens_count = len(synthesis_tokens)
            synthesis_tokens_numpy = mx.array(synthesis_tokens).tolist()
            
            final_synthesis = self.processor.tokenizer.decode(
                synthesis_tokens_numpy,
                skip_special_tokens=True
            ).strip()
            
            token_tracker.add_synthesis_tokens(input_tokens_count, synthesis_tokens_count)
            
            return f"# R√©sum√© Global de R√©union\n\n{final_synthesis}\n\n---\n\n## D√©tails par Segment\n\n{combined_content}"
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la synth√®se finale MLX: {e}")
            return f"# R√©sum√© de R√©union\n\n‚ö†Ô∏è Erreur lors de la synth√®se finale MLX: {str(e)}\n\n## Analyses par Segment\n\n{combined_content}"
    
    def cleanup_model(self):
        """
        Nettoie les ressources du mod√®le.
        """
        if self.model is not None:
            print("üßπ Nettoyage du mod√®le MLX...")
            # MLX g√®re automatiquement la m√©moire
            self.model = None
            self.processor = None
            print("‚úÖ Mod√®le MLX nettoy√©")